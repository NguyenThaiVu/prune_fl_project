{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will train the Federated Learning system with **ResNet architecture** on the FEMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:  tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e: print(e)\n",
    "\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config_celeb import *\n",
    "from utils.read_data_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.pruning_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Dataset Hyper-parameter\n",
    "DATASET_NAME = 'mnist'  # mnist\n",
    "\n",
    "IMAGE_DIMENSION = 28\n",
    "INPUT_SHAPE = (IMAGE_DIMENSION, IMAGE_DIMENSION, 1)\n",
    "\n",
    "OUPUT_SHAPE = 62 # \n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Model Hyper-parameter\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "LIST_NUMBER_FILTERS = [16, 32, 64]\n",
    "FILTER_SIZE = 5\n",
    "\n",
    "MODEL_TYPE = \"vanilla_conv\" # ['vanilla_conv', 'resnet', 'xception']\n",
    "PATH_GLOBAL_MODEL = os.path.join(\"models\", \"global_model_resnet_femnist_prune.h5\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Training Hyper-parameter\n",
    "NUM_ROUNDS = 500\n",
    "NUM_SELECTED_CLIENT = 10\n",
    "\n",
    "LOCAL_EPOCHS = 5\n",
    "LOCAL_BATCH_SIZE = 32\n",
    "\n",
    "MAX_PRUNED_ROUND = 100\n",
    "IS_STILL_PRUNE = True\n",
    "PRUNE_PATIENCE = 0\n",
    "MAX_PRUNE_PATIENCE = 3\n",
    "\n",
    "STD_THRESHOLD_PRUNE = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(only_digits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = len(emnist_train.client_ids)\n",
    "print(f\"Number of clients: {num_clients}\")\n",
    "\n",
    "list_num_samples = []\n",
    "for idx_client in range(num_clients):\n",
    "    num_samples = len(list(emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[idx_client])))\n",
    "    list_num_samples.append(num_samples)\n",
    "list_num_samples = np.array(list_num_samples)\n",
    "\n",
    "print(f\"Total number of samples in training set: {list_num_samples.sum()}\")\n",
    "print(f\"Average number of samples per client: {list_num_samples.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clients_data = Create_Clients_Data(emnist_train, DATASET_NAME)\n",
    "print(f\"Number of user: {len(list_clients_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_client = idx_sample = np.random.randint(0, 50)\n",
    "\n",
    "client_data = list_clients_data[idx_client]\n",
    "\n",
    "client_name = client_data['client_name']\n",
    "list_X = client_data['list_X']\n",
    "list_y = client_data['list_y']\n",
    "\n",
    "X = list_X[idx_sample]\n",
    "print(f\"Shape of image: {X.shape}\")\n",
    "y = list_y[idx_sample]\n",
    "\n",
    "print(f\"Client name= {client_name}\")\n",
    "print(f\"Label = {y}\")\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Prepare val - test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_test = Create_Clients_Data(emnist_test, dataset_name=DATASET_NAME)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for data_test in list_data_test:\n",
    "    X_test.append(data_test['list_X'])\n",
    "    y_test.append(data_test['list_y'])\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of X val: {X_val.shape}\")\n",
    "print(f\"Shape of y val: {y_val.shape}\")\n",
    "\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X val: {X_val.shape}\")\n",
    "print(f\"Shape of y val: {y_val.shape}\")\n",
    "print(f\"Max value of X_val: {X_val[3].max()}\")\n",
    "print(f\"Min value of X_val: {X_val[3].min()}\")\n",
    "print()\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FL Training\n",
    "\n",
    "## 2.1. Define components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Define_ResNet_Model(input_shape, output_shape, list_number_filters, max_pooling_step=2, model_name=None):\n",
    "    \"\"\"\n",
    "    This function create the simple Residual Network model. \n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial Convolutional Layer\n",
    "    x = layers.Conv2D(list_number_filters[0], kernel_size=3, strides=2, padding='same', name=f'prunable_conv_0')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=max_pooling_step, strides=max_pooling_step, padding='same')(x)\n",
    "    \n",
    "    # Residual Blocks\n",
    "    for (idx_residual_block, number_filters) in enumerate(list_number_filters[1:]):\n",
    "        x = residual_block(x, num_filters_1=number_filters, num_filters_2=number_filters, strides=(2, 2), idx_residual_block=idx_residual_block)\n",
    "    \n",
    "    # Final Layers\n",
    "    # x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = layers.Dense(output_shape, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "global_model = Define_ResNet_Model(INPUT_SHAPE, OUPUT_SHAPE, LIST_NUMBER_FILTERS, max_pooling_step=2, model_name=None)\n",
    "global_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics = METRICS)\n",
    "print(f\"Number of params: {global_model.count_params()}\")\n",
    "\n",
    "num_conv_layers = Count_Conv2d_Layers(global_model)\n",
    "print(f\"Number of Conv2D layer: {num_conv_layers}\")\n",
    "plot_model(global_model, to_file=os.path.join('images', f'ResNet_{num_conv_layers}_{DATASET_NAME}.png'), show_shapes=True, show_layer_names=True);\n",
    "global_model.save(PATH_GLOBAL_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. FL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, optimizer, loss_func, metrics, std_threshold=3.0):\n",
    "    \"\"\"\n",
    "    This function take input as model and perform model pruning to return the pruned filters CNN model.\n",
    "\n",
    "    * Parameters:\n",
    "        model (keras model): input model.\n",
    "        optimizer (keras optimizer).\n",
    "        loss_func (keras loss function).\n",
    "        metrics (keras metrics)\n",
    "        std_threshold (integer): threshold to prune filters.\n",
    "\n",
    "    * Return:\n",
    "        model (keras model) -- the pruned filters model.\n",
    "    \"\"\"\n",
    "\n",
    "    global IS_STILL_PRUNE\n",
    "    global PRUNE_PATIENCE\n",
    "    before_prune_params = model.count_params()\n",
    "\n",
    "    list_number_filters = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Conv2D) and layer.name != 'classifier' and 'prunable_conv' in layer.name:\n",
    "            weights = layer.get_weights()[0]\n",
    "            pruned_filter = Apply_Pruning_Filter(weights, std_threshold)\n",
    "            pruned_number_filter = pruned_filter.shape[-1]\n",
    "\n",
    "            if pruned_number_filter <= 0:\n",
    "                pruned_number_filter = 1\n",
    "            list_number_filters.append(pruned_number_filter)\n",
    "\n",
    "    new_model = Define_ResNet_Model(input_shape=model.input_shape[1:], output_shape=model.output_shape[1], list_number_filters=list_number_filters)\n",
    "    new_model_params = new_model.count_params()\n",
    "    print(f\"list_number_filters: {list_number_filters}\")\n",
    "    print(f\"before_prune_params: {before_prune_params}\")\n",
    "    print(f\"new_model_params: {new_model_params}\")\n",
    "\n",
    "    if before_prune_params > new_model_params:\n",
    "        PRUNE_PATIENCE = 0\n",
    "        print(f\"--- [INFO] This round PRUNE filter ---\")\n",
    "        new_model.compile(optimizer=optimizer, loss=loss_func, metrics=metrics)\n",
    "        return new_model\n",
    "    else:\n",
    "        PRUNE_PATIENCE += 1\n",
    "        print(f\"--- [INFO] This round NOT prune filter ---\")\n",
    "        if PRUNE_PATIENCE >= MAX_PRUNE_PATIENCE:\n",
    "            IS_STILL_PRUNE = False\n",
    "            print(f\"===== [INFO] Stop prune here! =====\")\n",
    "            print(f\"Final params: {before_prune_params}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = num_clients\n",
    "list_val_acc = []\n",
    "list_val_loss = []\n",
    "list_model_params = []\n",
    "\n",
    "\n",
    "for idx_round in range(NUM_ROUNDS):\n",
    "    print(\"\\n [INFO] Round {}\".format(idx_round))\n",
    "\n",
    "    # Load global model at begin of each round.\n",
    "    global_model = keras.models.load_model(PATH_GLOBAL_MODEL)\n",
    "    global_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics = METRICS)\n",
    "    \n",
    "    # Perform the pruning process\n",
    "    if IS_STILL_PRUNE == True:\n",
    "        if idx_round > MAX_PRUNED_ROUND:\n",
    "            IS_STILL_PRUNE = False\n",
    "            print(f\"===== [INFO] Stop prune here. Final params: {global_model.count_params()}\")\n",
    "        elif idx_round > 0:\n",
    "            global_model = prune_model(global_model, optimizer=OPTIMIZER, loss_func=LOSS, metrics=METRICS, std_threshold=STD_THRESHOLD_PRUNE)\n",
    "    \n",
    "    # Clients clone model from server\n",
    "    client_model = keras.models.clone_model(global_model)    \n",
    "    client_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "\n",
    "    selected_clients_data = random.sample(list_clients_data, NUM_SELECTED_CLIENT)  # Random subset clients\n",
    "\n",
    "    # Loop through selected client\n",
    "    list_client_model_weight = []\n",
    "    list_client_scales = []\n",
    "    for selectd_client_data in selected_clients_data:      \n",
    "\n",
    "        # Clone client's weight from global model\n",
    "        client_model.set_weights(global_model.get_weights())\n",
    "\n",
    "        client_name = selectd_client_data['client_name']\n",
    "        list_X = selectd_client_data['list_X']\n",
    "        list_y = selectd_client_data['list_y']\n",
    "        list_X = list_X.astype(np.float32) / 255.0\n",
    "        # list_X = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in list_X])  # Resize input image shape\n",
    "\n",
    "        client_model.fit(list_X, list_y, epochs=LOCAL_EPOCHS, batch_size=LOCAL_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        list_client_model_weight.append(client_model.get_weights())    # store local weight for update global model later.\n",
    "        list_client_scales.append(len(list_X))\n",
    "        list_X = list_y = None\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    # Calculate scale of each client\n",
    "    list_client_scales = np.array(list_client_scales)\n",
    "    list_client_scales = list_client_scales / list_client_scales.sum()\n",
    "\n",
    "    # Update the global model weights\n",
    "    avg_weights = FedAvg(global_model, list_client_model_weight, list_client_scales)\n",
    "    global_model.set_weights(avg_weights)\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    if idx_round % 20 == 0:\n",
    "        val_loss, val_acc = global_model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')\n",
    "        list_val_acc.append(val_acc)\n",
    "        list_val_loss.append(val_loss)\n",
    "\n",
    "    global_model.save(PATH_GLOBAL_MODEL)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    selected_clients_data = None\n",
    "    list_client_model_weight = list_client_scales = None\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in X_test])\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on testing data\n",
    "val_loss, val_acc = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== AFTER PRUNING: ==\")\n",
    "print(f\"Number of params: {global_model.count_params()}\")\n",
    "flops = get_flops_keras_model(global_model).as_integer_ratio()[0]\n",
    "print(f\"FLOPS of global model: {flops}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
