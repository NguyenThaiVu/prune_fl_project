{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will train the CNN model in the FL system. During the training, I will prune the filters of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:  tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e: print(e)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.read_data_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.pruning_utils import *\n",
    "from config_celeb import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_train, celeb_test = tff.simulation.datasets.celeba.load_data()\n",
    "\n",
    "num_clients = len(celeb_train.client_ids)\n",
    "print(f\"Number of clients: {num_clients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_clients_data = Create_Clients_Data(celeb_train, DATASET_NAME)\n",
    "print(f\"Number of client: {len(list_clients_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_client = np.random.randint(0, 10)\n",
    "idx_sample = np.random.randint(0, 10)\n",
    "\n",
    "client_data = list_clients_data[idx_client]\n",
    "\n",
    "client_name = client_data['client_name']\n",
    "list_X = client_data['list_X']\n",
    "list_y = client_data['list_y']\n",
    "\n",
    "X = list_X[idx_sample]\n",
    "print(f\"Shape of image: {X.shape}\")\n",
    "\n",
    "y = list_y[idx_sample]\n",
    "\n",
    "print(f\"Client = {client_name}\")\n",
    "print(f\"Label = {y}\")\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Prepare val - test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_test = Create_Clients_Data(celeb_test, dataset_name=DATASET_NAME)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for data_test in list_data_test:\n",
    "    X_test.append(data_test['list_X'])\n",
    "    y_test.append(data_test['list_y'])\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "X_val = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in X_val])\n",
    "X_test = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in X_test])\n",
    "\n",
    "print(f\"Shape of X val: {X_val.shape}\")\n",
    "print(f\"Shape of y val: {y_val.shape}\")\n",
    "\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training FL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Define components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_THRESHOLD_PRUNE = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# global_model = Define_ResNet_Model(input_shape=INPUT_SHAPE, output_shape=OUPUT_SHAPE, list_number_filters=LIST_NUMBER_FILTERS, model_name=\"global_model\")\n",
    "\n",
    "global_model = Get_Model(MODEL_TYPE, INPUT_SHAPE, OUPUT_SHAPE, LIST_NUMBER_FILTERS, model_name=\"global_model\")\n",
    "\n",
    "global_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics = METRICS)\n",
    "\n",
    "print(f\"Number of params: {global_model.count_params()}\")\n",
    "plot_model(global_model, to_file=os.path.join('images', f'model_architecture_{DATASET_NAME}.png'), show_shapes=True, show_layer_names=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_model = keras.models.clone_model(global_model)    \n",
    "client_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. FL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, optimizer, loss_func, metrics, std_threshold=2.0):\n",
    "    global IS_STILL_PRUNE\n",
    "    global PRUNE_PATIENCE\n",
    "    before_prune_params = model.count_params()\n",
    "\n",
    "    list_number_filters = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Conv2D) and layer.name != 'classifier' and 'prunable_conv' in layer.name:\n",
    "            weights = layer.get_weights()[0]\n",
    "            pruned_filter = Apply_Pruning_Filter(weights, std_threshold)\n",
    "            pruned_number_filter = pruned_filter.shape[-1]\n",
    "\n",
    "            if pruned_number_filter <= 0:\n",
    "                pruned_number_filter = 1\n",
    "            list_number_filters.append(pruned_number_filter)\n",
    "\n",
    "    # new_model = Define_ResNet_Model(input_shape=model.input_shape[1:], output_shape=model.output_shape[1], list_number_filters=list_number_filters)\n",
    "    new_model = Get_Model(MODEL_TYPE, input_shape=model.input_shape[1:], output_shape=model.output_shape[1], list_number_filters=list_number_filters)\n",
    "    new_model_params = new_model.count_params()\n",
    "\n",
    "    if before_prune_params > new_model_params:\n",
    "        PRUNE_PATIENCE = 0\n",
    "        print(f\"--- [INFO] This round PRUNE filter ---\")\n",
    "        new_model.compile(optimizer=optimizer, loss=loss_func, metrics=metrics)\n",
    "        return new_model\n",
    "    else:\n",
    "        PRUNE_PATIENCE += 1\n",
    "        print(f\"--- [INFO] This round NOT prune filter ---\")\n",
    "        if PRUNE_PATIENCE >= MAX_PRUNE_PATIENCE:\n",
    "            IS_STILL_PRUNE = False\n",
    "            print(f\"===== [INFO] Stop prune here! =====\")\n",
    "            print(f\"Final params: {before_prune_params}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = num_clients\n",
    "list_val_acc = []\n",
    "list_val_loss = []\n",
    "list_model_params = []\n",
    "\n",
    "\n",
    "for idx_round in range(NUM_ROUNDS):\n",
    "    print(\"\\n [INFO] Round {}\".format(idx_round))\n",
    "\n",
    "    if (idx_round > MAX_PRUNED_ROUND) and (IS_STILL_PRUNE == True):\n",
    "        IS_STILL_PRUNE = False\n",
    "        print(f\"===== [INFO] Stop prune here! =====\")\n",
    "        print(f\"Final params: {global_model.count_params()}\")\n",
    "\n",
    "    if (0 < idx_round) and (IS_STILL_PRUNE == True):  # Perform pruning\n",
    "        global_model = prune_model(global_model, optimizer=OPTIMIZER, loss_func=LOSS, metrics=METRICS, std_threshold=STD_THRESHOLD_PRUNE)\n",
    "        client_model = keras.models.clone_model(global_model)    \n",
    "        client_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "\n",
    "    # Select random subset of clients\n",
    "    num_selected_clients = int(NUM_CLIENTS * SELECTED_PERCENT_CLIENT)\n",
    "    selected_clients_data = random.sample(list_clients_data, num_selected_clients)\n",
    "\n",
    "    # Loop through selected client\n",
    "    list_client_model_weight = []\n",
    "    list_client_scales = []\n",
    "    for selectd_client_data in selected_clients_data:      \n",
    "\n",
    "        # Clone client's weight from global model\n",
    "        client_model.set_weights(global_model.get_weights())\n",
    "\n",
    "        client_name = selectd_client_data['client_name']\n",
    "        list_X = selectd_client_data['list_X']\n",
    "        list_y = selectd_client_data['list_y']\n",
    "        list_X = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in list_X])  # Resize input image shape\n",
    "\n",
    "        client_model.fit(list_X, list_y, epochs=LOCAL_EPOCHS, batch_size=LOCAL_BATCH_SIZE, verbose=0)\n",
    "\n",
    "        list_client_model_weight.append(client_model.get_weights())    # store local weight for update global model later.\n",
    "        list_client_scales.append(len(list_X))\n",
    "    \n",
    "    # Calculate scale of each client\n",
    "    list_client_scales = np.array(list_client_scales)\n",
    "    list_client_scales = list_client_scales / list_client_scales.sum()\n",
    "\n",
    "    # Update the global model weights\n",
    "    avg_weights = FedAvg(global_model, list_client_model_weight, list_client_scales)\n",
    "    global_model.set_weights(avg_weights)\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    val_loss, val_acc = global_model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')\n",
    "    list_val_acc.append(val_acc)\n",
    "    list_val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in X_test])\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on testing data\n",
    "val_loss, val_acc = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
