{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will train the CNN model in the FL system. During the training, I will prune the filters of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 23:02:17.171326: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 23:02:17.171344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 23:02:17.171363: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:  tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e: print(e)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config_femnist import *\n",
    "from utils.read_data_utils import *\n",
    "from utils.model_utils import *\n",
    "from utils.pruning_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(only_digits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 3400\n",
      "Total number of samples in training set: 671585\n",
      "Average number of samples per client: 197.525\n"
     ]
    }
   ],
   "source": [
    "num_clients = len(emnist_train.client_ids)\n",
    "print(f\"Number of clients: {num_clients}\")\n",
    "\n",
    "list_num_samples = []\n",
    "for idx_client in range(num_clients):\n",
    "    num_samples = len(list(emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[idx_client])))\n",
    "    list_num_samples.append(num_samples)\n",
    "list_num_samples = np.array(list_num_samples)\n",
    "\n",
    "print(f\"Total number of samples in training set: {list_num_samples.sum()}\")\n",
    "print(f\"Average number of samples per client: {list_num_samples.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user: 3400\n"
     ]
    }
   ],
   "source": [
    "list_clients_data = Create_Clients_Data(emnist_train, DATASET_NAME)\n",
    "print(f\"Number of user: {len(list_clients_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client: 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/necphy/miniconda3/envs/fl_env/lib/python3.9/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/home/necphy/miniconda3/envs/fl_env/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_num_samples = []\n",
    "\n",
    "for client_data in list_clients_data:\n",
    "    list_X = client_data['list_X']\n",
    "    list_num_samples.append(len(list_X))\n",
    "\n",
    "print(f\"Number of client: {len(list_num_samples)}\")\n",
    "plt.title(\"Histogram of number of samples per client\")\n",
    "sns.histplot(list_num_samples)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image: (28, 28, 1)\n",
      "Client name= f0052_42\n",
      "Label = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_client = idx_sample = np.random.randint(0, 50)\n",
    "\n",
    "client_data = list_clients_data[idx_client]\n",
    "\n",
    "client_name = client_data['client_name']\n",
    "list_X = client_data['list_X']\n",
    "list_y = client_data['list_y']\n",
    "\n",
    "X = list_X[idx_sample]\n",
    "print(f\"Shape of image: {X.shape}\")\n",
    "y = list_y[idx_sample]\n",
    "\n",
    "print(f\"Client name= {client_name}\")\n",
    "print(f\"Label = {y}\")\n",
    "plt.imshow(X, cmap='gray')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Prepare val - test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X val: (38741, 28, 28, 1)\n",
      "Shape of y val: (38741, 62)\n",
      "Shape of X test: (38742, 28, 28, 1)\n",
      "Shape of y test: (38742, 62)\n"
     ]
    }
   ],
   "source": [
    "list_data_test = Create_Clients_Data(emnist_test)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for data_test in list_data_test:\n",
    "    X_test.append(data_test['list_X'])\n",
    "    y_test.append(data_test['list_y'])\n",
    "X_test = np.concatenate(X_test)\n",
    "y_test = np.concatenate(y_test)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of X val: {X_val.shape}\")\n",
    "print(f\"Shape of y val: {y_val.shape}\")\n",
    "\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training FL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"global_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prunable_conv_0 (Conv2D)    (None, 24, 24, 32)        832       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " prunable_conv_1 (Conv2D)    (None, 8, 8, 64)          51264     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 64)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " classifier (Dense)          (None, 62)                4030      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56126 (219.24 KB)\n",
      "Trainable params: 56126 (219.24 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Number of params: 56126\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "global_model = Define_Simple_CNN_Model(input_shape=INPUT_SHAPE, output_shape=OUPUT_SHAPE,\\\n",
    "                            list_number_filters=LIST_NUMBER_FILTERS, model_name=\"global_model\")\n",
    "\n",
    "# global_model = Get_Model(MODEL_TYPE, INPUT_SHAPE, OUPUT_SHAPE, LIST_NUMBER_FILTERS, model_name=\"global_model\")\n",
    "\n",
    "global_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics = METRICS)\n",
    "global_model.summary()\n",
    "print(f\"Number of params: {global_model.count_params()}\")\n",
    "plot_model(global_model, to_file=os.path.join('images', f'model_architecture_{DATASET_NAME}.png'), show_shapes=True, show_layer_names=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_model = keras.models.clone_model(global_model)    \n",
    "client_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, optimizer, loss_func, metrics, std_threshold=3.0):\n",
    "    \"\"\"\n",
    "    This function take input as model and perform model pruning to return the pruned filters CNN model.\n",
    "\n",
    "    * Parameters:\n",
    "        model (keras model): input model.\n",
    "        optimizer (keras optimizer).\n",
    "        loss_func (keras loss function).\n",
    "        metrics (keras metrics)\n",
    "        std_threshold (integer): threshold to prune filters.\n",
    "\n",
    "    * Return:\n",
    "        model (keras model) -- the pruned filters model.\n",
    "    \"\"\"\n",
    "\n",
    "    global IS_STILL_PRUNE\n",
    "    global PRUNE_PATIENCE\n",
    "    before_prune_params = model.count_params()\n",
    "\n",
    "    list_number_filters = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Conv2D) and layer.name != 'classifier':\n",
    "            weights = layer.get_weights()[0]\n",
    "            pruned_filter = Apply_Pruning_Filter(weights, std_threshold)\n",
    "            pruned_number_filter = pruned_filter.shape[-1]\n",
    "\n",
    "            if pruned_number_filter <= 0:\n",
    "                pruned_number_filter = 1\n",
    "            list_number_filters.append(pruned_number_filter)\n",
    "\n",
    "    new_model = Define_Simple_CNN_Model(input_shape=model.input_shape[1:], output_shape=model.output_shape[1], list_number_filters=list_number_filters)\n",
    "    new_model_params = new_model.count_params()\n",
    "\n",
    "    if before_prune_params > new_model_params:\n",
    "        PRUNE_PATIENCE = 0\n",
    "        print(f\"--- [INFO] This round PRUNE filter ---\")\n",
    "        new_model.compile(optimizer=optimizer, loss=loss_func, metrics=metrics)\n",
    "        return new_model\n",
    "    else:\n",
    "        PRUNE_PATIENCE += 1\n",
    "        print(f\"--- [INFO] This round NOT prune filter ---\")\n",
    "        if PRUNE_PATIENCE >= MAX_PRUNE_PATIENCE:\n",
    "            IS_STILL_PRUNE = False\n",
    "            print(f\"===== [INFO] Stop prune here! =====\")\n",
    "            print(f\"Final params: {before_prune_params}\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Round 0\n",
      "Val loss: 3.7482378482818604, Val accuracy: 0.10719908773899078\n",
      "\n",
      " [INFO] Round 1\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8057992458343506, Val accuracy: 0.05368988960981369\n",
      "\n",
      " [INFO] Round 2\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7799489498138428, Val accuracy: 0.10397253185510635\n",
      "\n",
      " [INFO] Round 3\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7844717502593994, Val accuracy: 0.04966314882040024\n",
      "\n",
      " [INFO] Round 4\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6646296977996826, Val accuracy: 0.08334839344024658\n",
      "\n",
      " [INFO] Round 5\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.757321357727051, Val accuracy: 0.0504891462624073\n",
      "\n",
      " [INFO] Round 6\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8175511360168457, Val accuracy: 0.04966314882040024\n",
      "\n",
      " [INFO] Round 7\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6667027473449707, Val accuracy: 0.10100410133600235\n",
      "\n",
      " [INFO] Round 8\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.5905134677886963, Val accuracy: 0.12756511569023132\n",
      "\n",
      " [INFO] Round 9\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8188416957855225, Val accuracy: 0.0504891462624073\n",
      "\n",
      " [INFO] Round 10\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.774883508682251, Val accuracy: 0.07875377684831619\n",
      "\n",
      " [INFO] Round 11\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.822402000427246, Val accuracy: 0.0831160768866539\n",
      "\n",
      " [INFO] Round 12\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.682006359100342, Val accuracy: 0.05792313069105148\n",
      "\n",
      " [INFO] Round 13\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7889270782470703, Val accuracy: 0.04881133511662483\n",
      "\n",
      " [INFO] Round 14\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8115501403808594, Val accuracy: 0.05221857875585556\n",
      "\n",
      " [INFO] Round 15\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6691641807556152, Val accuracy: 0.07031310349702835\n",
      "\n",
      " [INFO] Round 16\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6175191402435303, Val accuracy: 0.11762732267379761\n",
      "\n",
      " [INFO] Round 17\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.83640718460083, Val accuracy: 0.05095376819372177\n",
      "\n",
      " [INFO] Round 18\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.820124864578247, Val accuracy: 0.05340595170855522\n",
      "\n",
      " [INFO] Round 19\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7725753784179688, Val accuracy: 0.04881133511662483\n",
      "\n",
      " [INFO] Round 20\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.769427537918091, Val accuracy: 0.04643659293651581\n",
      "\n",
      " [INFO] Round 21\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.857607364654541, Val accuracy: 0.04896621033549309\n",
      "\n",
      " [INFO] Round 22\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8195815086364746, Val accuracy: 0.05596138536930084\n",
      "\n",
      " [INFO] Round 23\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7518069744110107, Val accuracy: 0.054619137197732925\n",
      "\n",
      " [INFO] Round 24\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8390214443206787, Val accuracy: 0.05387057736515999\n",
      "\n",
      " [INFO] Round 25\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.846724033355713, Val accuracy: 0.05033427104353905\n",
      "\n",
      " [INFO] Round 26\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.866950511932373, Val accuracy: 0.013215973041951656\n",
      "\n",
      " [INFO] Round 27\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6892507076263428, Val accuracy: 0.10221728682518005\n",
      "\n",
      " [INFO] Round 28\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7591552734375, Val accuracy: 0.04896621033549309\n",
      "\n",
      " [INFO] Round 29\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7640504837036133, Val accuracy: 0.05387057736515999\n",
      "\n",
      " [INFO] Round 30\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.866793632507324, Val accuracy: 0.04643659293651581\n",
      "\n",
      " [INFO] Round 31\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.7731828689575195, Val accuracy: 0.04955989867448807\n",
      "\n",
      " [INFO] Round 32\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8563461303710938, Val accuracy: 0.033685244619846344\n",
      "\n",
      " [INFO] Round 33\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.76436710357666, Val accuracy: 0.05229601636528969\n",
      "\n",
      " [INFO] Round 34\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.67781662940979, Val accuracy: 0.05818125605583191\n",
      "\n",
      " [INFO] Round 35\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6433920860290527, Val accuracy: 0.07005497813224792\n",
      "\n",
      " [INFO] Round 36\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.81527042388916, Val accuracy: 0.04966314882040024\n",
      "\n",
      " [INFO] Round 37\n",
      "--- [INFO] This round PRUNE filter ---\n",
      "Val loss: 3.8992621898651123, Val accuracy: 0.057432692497968674\n",
      "\n",
      " [INFO] Round 38\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.699507713317871, Val accuracy: 0.057432692497968674\n",
      "\n",
      " [INFO] Round 39\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "Val loss: 3.6519906520843506, Val accuracy: 0.06636380404233932\n",
      "\n",
      " [INFO] Round 40\n",
      "--- [INFO] This round NOT prune filter ---\n",
      "===== [INFO] Stop prune here! =====\n",
      "Final params: 21902\n",
      "Val loss: 3.616854667663574, Val accuracy: 0.1052115336060524\n",
      "\n",
      " [INFO] Round 41\n",
      "Val loss: 3.5771026611328125, Val accuracy: 0.16001135110855103\n",
      "\n",
      " [INFO] Round 42\n",
      "Val loss: 3.5143635272979736, Val accuracy: 0.19005703926086426\n",
      "\n",
      " [INFO] Round 43\n",
      "Val loss: 3.4311671257019043, Val accuracy: 0.21994785964488983\n",
      "\n",
      " [INFO] Round 44\n",
      "Val loss: 3.3447604179382324, Val accuracy: 0.21202343702316284\n",
      "\n",
      " [INFO] Round 45\n",
      "Val loss: 3.2405295372009277, Val accuracy: 0.24586355686187744\n",
      "\n",
      " [INFO] Round 46\n",
      "Val loss: 3.1483325958251953, Val accuracy: 0.2623060941696167\n",
      "\n",
      " [INFO] Round 47\n",
      "Val loss: 3.0656559467315674, Val accuracy: 0.2745153605937958\n",
      "\n",
      " [INFO] Round 48\n",
      "Val loss: 3.0022823810577393, Val accuracy: 0.2898479700088501\n",
      "\n",
      " [INFO] Round 49\n",
      "Val loss: 2.9212498664855957, Val accuracy: 0.3018507659435272\n",
      "\n",
      " [INFO] Round 50\n",
      "Val loss: 2.8493611812591553, Val accuracy: 0.3136470317840576\n",
      "\n",
      " [INFO] Round 51\n",
      "Val loss: 2.779874801635742, Val accuracy: 0.33256757259368896\n",
      "\n",
      " [INFO] Round 52\n",
      "Val loss: 2.751519203186035, Val accuracy: 0.33329030871391296\n",
      "\n",
      " [INFO] Round 53\n",
      "Val loss: 2.677664041519165, Val accuracy: 0.34859707951545715\n",
      "\n",
      " [INFO] Round 54\n",
      "Val loss: 2.6247572898864746, Val accuracy: 0.3624841868877411\n",
      "\n",
      " [INFO] Round 55\n",
      "Val loss: 2.5819201469421387, Val accuracy: 0.37141528725624084\n",
      "\n",
      " [INFO] Round 56\n",
      "Val loss: 2.5242204666137695, Val accuracy: 0.3908778727054596\n",
      "\n",
      " [INFO] Round 57\n",
      "Val loss: 2.5029330253601074, Val accuracy: 0.3871867060661316\n",
      "\n",
      " [INFO] Round 58\n",
      "Val loss: 2.4600374698638916, Val accuracy: 0.39949923753738403\n",
      "\n",
      " [INFO] Round 59\n",
      "Val loss: 2.419422149658203, Val accuracy: 0.41565781831741333\n",
      "\n",
      " [INFO] Round 60\n",
      "Val loss: 2.3842930793762207, Val accuracy: 0.4143155813217163\n",
      "\n",
      " [INFO] Round 61\n",
      "Val loss: 2.3459463119506836, Val accuracy: 0.4225497543811798\n",
      "\n",
      " [INFO] Round 62\n",
      "Val loss: 2.301299810409546, Val accuracy: 0.4323326647281647\n",
      "\n",
      " [INFO] Round 63\n",
      "Val loss: 2.2780587673187256, Val accuracy: 0.4353785514831543\n",
      "\n",
      " [INFO] Round 64\n",
      "Val loss: 2.2453083992004395, Val accuracy: 0.44467103481292725\n",
      "\n",
      " [INFO] Round 65\n",
      "Val loss: 2.202554702758789, Val accuracy: 0.45538318157196045\n",
      "\n",
      " [INFO] Round 66\n",
      "Val loss: 2.1759042739868164, Val accuracy: 0.4601326882839203\n",
      "\n",
      " [INFO] Round 67\n",
      "Val loss: 2.149075984954834, Val accuracy: 0.4656565487384796\n",
      "\n",
      " [INFO] Round 68\n",
      "Val loss: 2.1301279067993164, Val accuracy: 0.46779897809028625\n",
      "\n",
      " [INFO] Round 69\n",
      "Val loss: 2.0943875312805176, Val accuracy: 0.4768591523170471\n",
      "\n",
      " [INFO] Round 70\n",
      "Val loss: 2.0582077503204346, Val accuracy: 0.48576444387435913\n",
      "\n",
      " [INFO] Round 71\n",
      "Val loss: 2.0332112312316895, Val accuracy: 0.49224334955215454\n",
      "\n",
      " [INFO] Round 72\n",
      "Val loss: 2.011408567428589, Val accuracy: 0.49660566449165344\n",
      "\n",
      " [INFO] Round 73\n",
      "Val loss: 1.998094081878662, Val accuracy: 0.5002968311309814\n",
      "\n",
      " [INFO] Round 74\n",
      "Val loss: 1.9651122093200684, Val accuracy: 0.5064918398857117\n",
      "\n",
      " [INFO] Round 75\n",
      "Val loss: 1.9407243728637695, Val accuracy: 0.5140290856361389\n",
      "\n",
      " [INFO] Round 76\n",
      "Val loss: 1.9198077917099, Val accuracy: 0.5135644674301147\n",
      "\n",
      " [INFO] Round 77\n",
      "Val loss: 1.903296947479248, Val accuracy: 0.5184687972068787\n",
      "\n",
      " [INFO] Round 78\n",
      "Val loss: 1.875354528427124, Val accuracy: 0.5281742811203003\n",
      "\n",
      " [INFO] Round 79\n",
      "Val loss: 1.8582203388214111, Val accuracy: 0.5310652852058411\n",
      "\n",
      " [INFO] Round 80\n",
      "Val loss: 1.8433257341384888, Val accuracy: 0.5282258987426758\n",
      "\n",
      " [INFO] Round 81\n",
      "Val loss: 1.819272756576538, Val accuracy: 0.5349371433258057\n",
      "\n",
      " [INFO] Round 82\n",
      "Val loss: 1.8056201934814453, Val accuracy: 0.5395059585571289\n",
      "\n",
      " [INFO] Round 83\n",
      "Val loss: 1.7829821109771729, Val accuracy: 0.5451846718788147\n",
      "\n",
      " [INFO] Round 84\n",
      "Val loss: 1.7892512083053589, Val accuracy: 0.5443844795227051\n",
      "\n",
      " [INFO] Round 85\n",
      "Val loss: 1.7631865739822388, Val accuracy: 0.5471980571746826\n",
      "\n",
      " [INFO] Round 86\n",
      "Val loss: 1.7506300210952759, Val accuracy: 0.5501406788825989\n",
      "\n",
      " [INFO] Round 87\n",
      "Val loss: 1.7326979637145996, Val accuracy: 0.5555354952812195\n",
      "\n",
      " [INFO] Round 88\n",
      "Val loss: 1.7238481044769287, Val accuracy: 0.5551483035087585\n",
      "\n",
      " [INFO] Round 89\n",
      "Val loss: 1.7011626958847046, Val accuracy: 0.5636405944824219\n",
      "\n",
      " [INFO] Round 90\n",
      "Val loss: 1.6881892681121826, Val accuracy: 0.5644665956497192\n",
      "\n",
      " [INFO] Round 91\n",
      "Val loss: 1.6762433052062988, Val accuracy: 0.5664541721343994\n",
      "\n",
      " [INFO] Round 92\n",
      "Val loss: 1.6667022705078125, Val accuracy: 0.5673317909240723\n",
      "\n",
      " [INFO] Round 93\n",
      "Val loss: 1.6550920009613037, Val accuracy: 0.5708680748939514\n",
      "\n",
      " [INFO] Round 94\n",
      "Val loss: 1.6414368152618408, Val accuracy: 0.5754885077476501\n",
      "\n",
      " [INFO] Round 95\n",
      "Val loss: 1.6358141899108887, Val accuracy: 0.5748947858810425\n",
      "\n",
      " [INFO] Round 96\n",
      "Val loss: 1.6325355768203735, Val accuracy: 0.5779665112495422\n",
      "\n",
      " [INFO] Round 97\n",
      "Val loss: 1.6134836673736572, Val accuracy: 0.5791797041893005\n",
      "\n",
      " [INFO] Round 98\n",
      "Val loss: 1.6017968654632568, Val accuracy: 0.5830257534980774\n",
      "\n",
      " [INFO] Round 99\n",
      "Val loss: 1.5902409553527832, Val accuracy: 0.585477888584137\n",
      "\n",
      " [INFO] Round 100\n",
      "Val loss: 1.5853341817855835, Val accuracy: 0.5872331857681274\n",
      "\n",
      " [INFO] Round 101\n",
      "Val loss: 1.607478141784668, Val accuracy: 0.5822255611419678\n",
      "\n",
      " [INFO] Round 102\n",
      "Val loss: 1.5786556005477905, Val accuracy: 0.5891948938369751\n",
      "\n",
      " [INFO] Round 103\n",
      "Val loss: 1.5830450057983398, Val accuracy: 0.5895046591758728\n",
      "\n",
      " [INFO] Round 104\n",
      "Val loss: 1.5551661252975464, Val accuracy: 0.5928602814674377\n",
      "\n",
      " [INFO] Round 105\n",
      "Val loss: 1.5426976680755615, Val accuracy: 0.5959835648536682\n",
      "\n",
      " [INFO] Round 106\n",
      "Val loss: 1.5333539247512817, Val accuracy: 0.5973774790763855\n",
      "\n",
      " [INFO] Round 107\n",
      "Val loss: 1.5354050397872925, Val accuracy: 0.5969644784927368\n",
      "\n",
      " [INFO] Round 108\n",
      "Val loss: 1.518581509590149, Val accuracy: 0.6011977195739746\n",
      "\n",
      " [INFO] Round 109\n",
      "Val loss: 1.5112416744232178, Val accuracy: 0.6023334264755249\n",
      "\n",
      " [INFO] Round 110\n",
      "Val loss: 1.5303497314453125, Val accuracy: 0.5959319472312927\n",
      "\n",
      " [INFO] Round 111\n",
      "Val loss: 1.5759724378585815, Val accuracy: 0.5942283272743225\n",
      "\n",
      " [INFO] Round 112\n",
      "Val loss: 1.4982285499572754, Val accuracy: 0.6035466194152832\n",
      "\n",
      " [INFO] Round 113\n",
      "Val loss: 1.4871495962142944, Val accuracy: 0.6073926687240601\n",
      "\n",
      " [INFO] Round 114\n",
      "Val loss: 1.47745943069458, Val accuracy: 0.6106192469596863\n",
      "\n",
      " [INFO] Round 115\n",
      "Val loss: 1.4919378757476807, Val accuracy: 0.6033401489257812\n",
      "\n",
      " [INFO] Round 116\n",
      "Val loss: 1.4676765203475952, Val accuracy: 0.6130972504615784\n",
      "\n",
      " [INFO] Round 117\n",
      "Val loss: 1.4735733270645142, Val accuracy: 0.6144136786460876\n",
      "\n",
      " [INFO] Round 118\n",
      "Val loss: 1.45641028881073, Val accuracy: 0.6147234439849854\n",
      "\n",
      " [INFO] Round 119\n",
      "Val loss: 1.4504188299179077, Val accuracy: 0.6157817244529724\n",
      "\n",
      " [INFO] Round 120\n",
      "Val loss: 1.447556495666504, Val accuracy: 0.6158333420753479\n",
      "\n",
      " [INFO] Round 121\n",
      "Val loss: 1.452466607093811, Val accuracy: 0.6153945326805115\n",
      "\n",
      " [INFO] Round 122\n",
      "Val loss: 1.4361387491226196, Val accuracy: 0.6194213032722473\n",
      "\n",
      " [INFO] Round 123\n",
      "Val loss: 1.4245551824569702, Val accuracy: 0.6231899261474609\n",
      "\n",
      " [INFO] Round 124\n",
      "Val loss: 1.4450980424880981, Val accuracy: 0.6187759637832642\n",
      "\n",
      " [INFO] Round 125\n",
      "Val loss: 1.4220823049545288, Val accuracy: 0.6218993067741394\n",
      "\n",
      " [INFO] Round 126\n",
      "Val loss: 1.4138362407684326, Val accuracy: 0.6247386336326599\n",
      "\n",
      " [INFO] Round 127\n",
      "Val loss: 1.4130620956420898, Val accuracy: 0.6231641173362732\n",
      "\n",
      " [INFO] Round 128\n",
      "Val loss: 1.4048287868499756, Val accuracy: 0.624248206615448\n",
      "\n",
      " [INFO] Round 129\n",
      "Val loss: 1.4455480575561523, Val accuracy: 0.6207119226455688\n",
      "\n",
      " [INFO] Round 130\n",
      "Val loss: 1.3937246799468994, Val accuracy: 0.6293332576751709\n",
      "\n",
      " [INFO] Round 131\n",
      "Val loss: 1.392949104309082, Val accuracy: 0.6305206418037415\n",
      "\n",
      " [INFO] Round 132\n",
      "Val loss: 1.3892951011657715, Val accuracy: 0.6301334500312805\n",
      "\n",
      " [INFO] Round 133\n",
      "Val loss: 1.3948415517807007, Val accuracy: 0.6274231672286987\n",
      "\n",
      " [INFO] Round 134\n",
      "Val loss: 1.388578176498413, Val accuracy: 0.629126787185669\n",
      "\n",
      " [INFO] Round 135\n",
      "Val loss: 1.3725460767745972, Val accuracy: 0.6336697340011597\n",
      "\n",
      " [INFO] Round 136\n",
      "Val loss: 1.3707364797592163, Val accuracy: 0.6336697340011597\n",
      "\n",
      " [INFO] Round 137\n",
      "Val loss: 1.364261269569397, Val accuracy: 0.6359928846359253\n",
      "\n",
      " [INFO] Round 138\n",
      "Val loss: 1.3621771335601807, Val accuracy: 0.6336955428123474\n",
      "\n",
      " [INFO] Round 139\n",
      "Val loss: 1.354370355606079, Val accuracy: 0.6389613151550293\n",
      "\n",
      " [INFO] Round 140\n",
      "Val loss: 1.3586632013320923, Val accuracy: 0.6381353139877319\n",
      "\n",
      " [INFO] Round 141\n",
      "Val loss: 1.3493720293045044, Val accuracy: 0.6389096975326538\n",
      "\n",
      " [INFO] Round 142\n",
      "Val loss: 1.34349524974823, Val accuracy: 0.6415683627128601\n",
      "\n",
      " [INFO] Round 143\n",
      "Val loss: 1.3405919075012207, Val accuracy: 0.6391419768333435\n",
      "\n",
      " [INFO] Round 144\n",
      "Val loss: 1.3405758142471313, Val accuracy: 0.6363284587860107\n",
      "\n",
      " [INFO] Round 145\n",
      "Val loss: 1.3273377418518066, Val accuracy: 0.6431687474250793\n",
      "\n",
      " [INFO] Round 146\n",
      "Val loss: 1.3483713865280151, Val accuracy: 0.6386773586273193\n",
      "\n",
      " [INFO] Round 147\n",
      "Val loss: 1.3207811117172241, Val accuracy: 0.6439947485923767\n",
      "\n",
      " [INFO] Round 148\n",
      "Val loss: 1.3218467235565186, Val accuracy: 0.6456467509269714\n",
      "\n",
      " [INFO] Round 149\n",
      "Val loss: 1.3168203830718994, Val accuracy: 0.6458274126052856\n",
      "\n",
      " [INFO] Round 150\n",
      "Val loss: 1.315967082977295, Val accuracy: 0.645207941532135\n",
      "\n",
      " [INFO] Round 151\n",
      "Val loss: 1.3117707967758179, Val accuracy: 0.6470664143562317\n",
      "\n",
      " [INFO] Round 152\n",
      "Val loss: 1.3109588623046875, Val accuracy: 0.645311176776886\n",
      "\n",
      " [INFO] Round 153\n",
      "Val loss: 1.3163559436798096, Val accuracy: 0.6443819403648376\n",
      "\n",
      " [INFO] Round 154\n",
      "Val loss: 1.3027279376983643, Val accuracy: 0.649337887763977\n",
      "\n",
      " [INFO] Round 155\n",
      "Val loss: 1.302579402923584, Val accuracy: 0.6490023732185364\n",
      "\n",
      " [INFO] Round 156\n",
      "Val loss: 1.297910451889038, Val accuracy: 0.6469889879226685\n",
      "\n",
      " [INFO] Round 157\n",
      "Val loss: 1.2876994609832764, Val accuracy: 0.6520224213600159\n",
      "\n",
      " [INFO] Round 158\n",
      "Val loss: 1.2894455194473267, Val accuracy: 0.6491314172744751\n",
      "\n",
      " [INFO] Round 159\n",
      "Val loss: 1.2873659133911133, Val accuracy: 0.6498283743858337\n",
      "\n",
      " [INFO] Round 160\n",
      "Val loss: 1.2905638217926025, Val accuracy: 0.6496734619140625\n",
      "\n",
      " [INFO] Round 161\n",
      "Val loss: 1.2758044004440308, Val accuracy: 0.6552489399909973\n",
      "\n",
      " [INFO] Round 162\n",
      "Val loss: 1.27632474899292, Val accuracy: 0.6527968049049377\n",
      "\n",
      " [INFO] Round 163\n",
      "Val loss: 1.2688653469085693, Val accuracy: 0.6563330888748169\n",
      "\n",
      " [INFO] Round 164\n",
      "Val loss: 1.2767877578735352, Val accuracy: 0.6546036601066589\n",
      "\n",
      " [INFO] Round 165\n",
      "Val loss: 1.268349051475525, Val accuracy: 0.6564879417419434\n",
      "\n",
      " [INFO] Round 166\n",
      "Val loss: 1.2616299390792847, Val accuracy: 0.6583206653594971\n",
      "\n",
      " [INFO] Round 167\n",
      "Val loss: 1.2546837329864502, Val accuracy: 0.6604889035224915\n",
      "\n",
      " [INFO] Round 168\n",
      "Val loss: 1.2711788415908813, Val accuracy: 0.6552489399909973\n",
      "\n",
      " [INFO] Round 169\n",
      "Val loss: 1.2566514015197754, Val accuracy: 0.6599984765052795\n",
      "\n",
      " [INFO] Round 170\n",
      "Val loss: 1.2604320049285889, Val accuracy: 0.6577269434928894\n",
      "\n",
      " [INFO] Round 171\n",
      "Val loss: 1.251062273979187, Val accuracy: 0.6613665223121643\n",
      "\n",
      " [INFO] Round 172\n",
      "Val loss: 1.246200680732727, Val accuracy: 0.6614697575569153\n",
      "\n",
      " [INFO] Round 173\n",
      "Val loss: 1.2431085109710693, Val accuracy: 0.6633540987968445\n",
      "\n",
      " [INFO] Round 174\n",
      "Val loss: 1.2559630870819092, Val accuracy: 0.6599726676940918\n",
      "\n",
      " [INFO] Round 175\n",
      "Val loss: 1.239970326423645, Val accuracy: 0.6637154221534729\n",
      "\n",
      " [INFO] Round 176\n",
      "Val loss: 1.2424554824829102, Val accuracy: 0.6636896133422852\n",
      "\n",
      " [INFO] Round 177\n",
      "Val loss: 1.2317640781402588, Val accuracy: 0.6650577187538147\n",
      "\n",
      " [INFO] Round 178\n",
      "Val loss: 1.2283120155334473, Val accuracy: 0.6667096614837646\n",
      "\n",
      " [INFO] Round 179\n",
      "Val loss: 1.226616382598877, Val accuracy: 0.6663483381271362\n",
      "\n",
      " [INFO] Round 180\n",
      "Val loss: 1.2292598485946655, Val accuracy: 0.6657288074493408\n",
      "\n",
      " [INFO] Round 181\n",
      "Val loss: 1.2202882766723633, Val accuracy: 0.6682326197624207\n",
      "\n",
      " [INFO] Round 182\n",
      "Val loss: 1.2196762561798096, Val accuracy: 0.667716383934021\n",
      "\n",
      " [INFO] Round 183\n",
      "Val loss: 1.223542332649231, Val accuracy: 0.6665548086166382\n",
      "\n",
      " [INFO] Round 184\n",
      "Val loss: 1.2151237726211548, Val accuracy: 0.6693683862686157\n",
      "\n",
      " [INFO] Round 185\n",
      "Val loss: 1.2177826166152954, Val accuracy: 0.6686972379684448\n",
      "\n",
      " [INFO] Round 186\n",
      "Val loss: 1.2120792865753174, Val accuracy: 0.667716383934021\n",
      "\n",
      " [INFO] Round 187\n",
      "Val loss: 1.2103673219680786, Val accuracy: 0.6714850068092346\n",
      "\n",
      " [INFO] Round 188\n",
      "Val loss: 1.210883617401123, Val accuracy: 0.670400857925415\n",
      "\n",
      " [INFO] Round 189\n",
      "Val loss: 1.2040232419967651, Val accuracy: 0.672233521938324\n",
      "\n",
      " [INFO] Round 190\n",
      "Val loss: 1.207446575164795, Val accuracy: 0.6706590056419373\n",
      "\n",
      " [INFO] Round 191\n",
      "Val loss: 1.1994621753692627, Val accuracy: 0.6726723909378052\n",
      "\n",
      " [INFO] Round 192\n",
      "Val loss: 1.2049418687820435, Val accuracy: 0.6689037680625916\n",
      "\n",
      " [INFO] Round 193\n",
      "Val loss: 1.1959327459335327, Val accuracy: 0.67347252368927\n",
      "\n",
      " [INFO] Round 194\n",
      "Val loss: 1.1925991773605347, Val accuracy: 0.6754342913627625\n",
      "\n",
      " [INFO] Round 195\n",
      "Val loss: 1.2074259519577026, Val accuracy: 0.6717172861099243\n",
      "\n",
      " [INFO] Round 196\n",
      "Val loss: 1.2165969610214233, Val accuracy: 0.6665031909942627\n",
      "\n",
      " [INFO] Round 197\n",
      "Val loss: 1.1949234008789062, Val accuracy: 0.6737048625946045\n",
      "\n",
      " [INFO] Round 198\n",
      "Val loss: 1.1930153369903564, Val accuracy: 0.6747115254402161\n",
      "\n",
      " [INFO] Round 199\n",
      "Val loss: 1.189490556716919, Val accuracy: 0.6750729084014893\n",
      "\n",
      " [INFO] Round 200\n",
      "Val loss: 1.1868698596954346, Val accuracy: 0.6758472919464111\n",
      "\n",
      " [INFO] Round 201\n",
      "Val loss: 1.1865595579147339, Val accuracy: 0.6773444414138794\n",
      "\n",
      " [INFO] Round 202\n",
      "Val loss: 1.1819137334823608, Val accuracy: 0.6773702502250671\n",
      "\n",
      " [INFO] Round 203\n",
      "Val loss: 1.1788135766983032, Val accuracy: 0.677860677242279\n",
      "\n",
      " [INFO] Round 204\n",
      "Val loss: 1.1848641633987427, Val accuracy: 0.6735242009162903\n",
      "\n",
      " [INFO] Round 205\n",
      "Val loss: 1.1759192943572998, Val accuracy: 0.6788415312767029\n",
      "\n",
      " [INFO] Round 206\n",
      "Val loss: 1.1722749471664429, Val accuracy: 0.6799514889717102\n",
      "\n",
      " [INFO] Round 207\n",
      "Val loss: 1.1715601682662964, Val accuracy: 0.6802611947059631\n",
      "\n",
      " [INFO] Round 208\n",
      "Val loss: 1.1719204187393188, Val accuracy: 0.6802870631217957\n",
      "\n",
      " [INFO] Round 209\n",
      "Val loss: 1.166094422340393, Val accuracy: 0.681216299533844\n",
      "\n",
      " [INFO] Round 210\n",
      "Val loss: 1.1657122373580933, Val accuracy: 0.6797191500663757\n",
      "\n",
      " [INFO] Round 211\n",
      "Val loss: 1.1806294918060303, Val accuracy: 0.6736016273498535\n",
      "\n",
      " [INFO] Round 212\n",
      "Val loss: 1.1666368246078491, Val accuracy: 0.6797707676887512\n",
      "\n",
      " [INFO] Round 213\n",
      "Val loss: 1.1627742052078247, Val accuracy: 0.6789447665214539\n",
      "\n",
      " [INFO] Round 214\n",
      "Val loss: 1.152434229850769, Val accuracy: 0.6835651993751526\n",
      "\n",
      " [INFO] Round 215\n",
      "Val loss: 1.1513729095458984, Val accuracy: 0.6838491559028625\n",
      "\n",
      " [INFO] Round 216\n",
      "Val loss: 1.1500831842422485, Val accuracy: 0.6860173940658569\n",
      "\n",
      " [INFO] Round 217\n",
      "Val loss: 1.1502597332000732, Val accuracy: 0.6845203042030334\n",
      "\n",
      " [INFO] Round 218\n",
      "Val loss: 1.169472575187683, Val accuracy: 0.6803902983665466\n",
      "\n",
      " [INFO] Round 219\n",
      "Val loss: 1.145073652267456, Val accuracy: 0.6853721141815186\n",
      "\n",
      " [INFO] Round 220\n",
      "Val loss: 1.1474964618682861, Val accuracy: 0.6838491559028625\n",
      "\n",
      " [INFO] Round 221\n",
      "Val loss: 1.1414068937301636, Val accuracy: 0.686972439289093\n",
      "\n",
      " [INFO] Round 222\n",
      "Val loss: 1.1375125646591187, Val accuracy: 0.6873080134391785\n",
      "\n",
      " [INFO] Round 223\n",
      "Val loss: 1.1383122205734253, Val accuracy: 0.6881598234176636\n",
      "\n",
      " [INFO] Round 224\n",
      "Val loss: 1.2524141073226929, Val accuracy: 0.664851188659668\n",
      "\n",
      " [INFO] Round 225\n",
      "Val loss: 1.224225640296936, Val accuracy: 0.6687746644020081\n",
      "\n",
      " [INFO] Round 226\n",
      "Val loss: 1.1414722204208374, Val accuracy: 0.688495397567749\n",
      "\n",
      " [INFO] Round 227\n",
      "Val loss: 1.142640233039856, Val accuracy: 0.6869982481002808\n",
      "\n",
      " [INFO] Round 228\n",
      "Val loss: 1.1391918659210205, Val accuracy: 0.6872047781944275\n",
      "\n",
      " [INFO] Round 229\n",
      "Val loss: 1.1373287439346313, Val accuracy: 0.6883405447006226\n",
      "\n",
      " [INFO] Round 230\n",
      "Val loss: 1.1360660791397095, Val accuracy: 0.6868175864219666\n",
      "\n",
      " [INFO] Round 231\n",
      "Val loss: 1.1458461284637451, Val accuracy: 0.6846751570701599\n",
      "\n",
      " [INFO] Round 232\n",
      "Val loss: 1.1282659769058228, Val accuracy: 0.6909475922584534\n",
      "\n",
      " [INFO] Round 233\n",
      "Val loss: 1.1272321939468384, Val accuracy: 0.6901473999023438\n",
      "\n",
      " [INFO] Round 234\n",
      "Val loss: 1.1283347606658936, Val accuracy: 0.6893988251686096\n",
      "\n",
      " [INFO] Round 235\n",
      "Val loss: 1.2221097946166992, Val accuracy: 0.6702460050582886\n",
      "\n",
      " [INFO] Round 236\n",
      "Val loss: 1.125106692314148, Val accuracy: 0.6913089752197266\n",
      "\n",
      " [INFO] Round 237\n",
      "Val loss: 1.2427144050598145, Val accuracy: 0.6713300943374634\n",
      "\n",
      " [INFO] Round 238\n",
      "Val loss: 1.1318508386611938, Val accuracy: 0.6893472075462341\n",
      "\n",
      " [INFO] Round 239\n",
      "Val loss: 1.2433559894561768, Val accuracy: 0.655455470085144\n",
      "\n",
      " [INFO] Round 240\n",
      "Val loss: 1.1381319761276245, Val accuracy: 0.6900441646575928\n",
      "\n",
      " [INFO] Round 241\n",
      "Val loss: 1.186635971069336, Val accuracy: 0.6754601001739502\n",
      "\n",
      " [INFO] Round 242\n",
      "Val loss: 1.1188762187957764, Val accuracy: 0.6943032145500183\n",
      "\n",
      " [INFO] Round 243\n",
      "Val loss: 1.1175183057785034, Val accuracy: 0.6947936415672302\n",
      "\n",
      " [INFO] Round 244\n",
      "Val loss: 1.1112182140350342, Val accuracy: 0.6959810256958008\n",
      "\n",
      " [INFO] Round 245\n",
      "Val loss: 1.1083773374557495, Val accuracy: 0.6963940262794495\n",
      "\n",
      " [INFO] Round 246\n",
      "Val loss: 1.1105105876922607, Val accuracy: 0.6950517296791077\n",
      "\n",
      " [INFO] Round 247\n",
      "Val loss: 1.1106919050216675, Val accuracy: 0.6950259208679199\n",
      "\n",
      " [INFO] Round 248\n",
      "Val loss: 1.1063663959503174, Val accuracy: 0.696058452129364\n",
      "\n",
      " [INFO] Round 249\n",
      "Val loss: 1.1090483665466309, Val accuracy: 0.694638729095459\n",
      "\n",
      " [INFO] Round 250\n",
      "Val loss: 1.110721230506897, Val accuracy: 0.6945613026618958\n",
      "\n",
      " [INFO] Round 251\n",
      "Val loss: 1.106071949005127, Val accuracy: 0.6948968768119812\n",
      "\n",
      " [INFO] Round 252\n",
      "Val loss: 1.1088327169418335, Val accuracy: 0.6968070268630981\n",
      "\n",
      " [INFO] Round 253\n",
      "Val loss: 1.097856044769287, Val accuracy: 0.6975813508033752\n",
      "\n",
      " [INFO] Round 254\n",
      "Val loss: 1.099112868309021, Val accuracy: 0.6995173096656799\n",
      "\n",
      " [INFO] Round 255\n",
      "Val loss: 1.090850830078125, Val accuracy: 0.7004465460777283\n",
      "\n",
      " [INFO] Round 256\n",
      "Val loss: 1.251685380935669, Val accuracy: 0.6623215675354004\n",
      "\n",
      " [INFO] Round 257\n",
      "Val loss: 1.0957767963409424, Val accuracy: 0.698923647403717\n",
      "\n",
      " [INFO] Round 258\n",
      "Val loss: 1.096197485923767, Val accuracy: 0.6979943513870239\n",
      "\n",
      " [INFO] Round 259\n",
      "Val loss: 1.091772437095642, Val accuracy: 0.6999045014381409\n",
      "\n",
      " [INFO] Round 260\n",
      "Val loss: 1.0920231342315674, Val accuracy: 0.6997754573822021\n",
      "\n",
      " [INFO] Round 261\n",
      "Val loss: 1.1048922538757324, Val accuracy: 0.6985106468200684\n",
      "\n",
      " [INFO] Round 262\n",
      "Val loss: 1.0881893634796143, Val accuracy: 0.7026922106742859\n",
      "\n",
      " [INFO] Round 263\n",
      "Val loss: 1.0876120328903198, Val accuracy: 0.7017629742622375\n",
      "\n",
      " [INFO] Round 264\n",
      "Val loss: 1.0897058248519897, Val accuracy: 0.7012209296226501\n",
      "\n",
      " [INFO] Round 265\n",
      "Val loss: 1.1054197549819946, Val accuracy: 0.6974781155586243\n",
      "\n",
      " [INFO] Round 266\n",
      "Val loss: 1.0818614959716797, Val accuracy: 0.7029761672019958\n",
      "\n",
      " [INFO] Round 267\n",
      "Val loss: 1.102760910987854, Val accuracy: 0.6993624567985535\n",
      "\n",
      " [INFO] Round 268\n",
      "Val loss: 1.0841305255889893, Val accuracy: 0.7022792100906372\n",
      "\n",
      " [INFO] Round 269\n",
      "Val loss: 1.085331916809082, Val accuracy: 0.7013499736785889\n",
      "\n",
      " [INFO] Round 270\n",
      "Val loss: 2.0979998111724854, Val accuracy: 0.5499858260154724\n",
      "\n",
      " [INFO] Round 271\n",
      "Val loss: 1.0926120281219482, Val accuracy: 0.6971425414085388\n",
      "\n",
      " [INFO] Round 272\n",
      "Val loss: 1.074159860610962, Val accuracy: 0.7026664018630981\n",
      "\n",
      " [INFO] Round 273\n",
      "Val loss: 1.0748902559280396, Val accuracy: 0.7027955055236816\n",
      "\n",
      " [INFO] Round 274\n",
      "Val loss: 1.0667401552200317, Val accuracy: 0.706512451171875\n",
      "\n",
      " [INFO] Round 275\n",
      "Val loss: 1.0802968740463257, Val accuracy: 0.7027180790901184\n",
      "\n",
      " [INFO] Round 276\n",
      "Val loss: 1.093734622001648, Val accuracy: 0.6982783079147339\n",
      "\n",
      " [INFO] Round 277\n",
      "Val loss: 1.0630627870559692, Val accuracy: 0.7055832147598267\n",
      "\n",
      " [INFO] Round 278\n",
      "Val loss: 1.0624475479125977, Val accuracy: 0.7054283618927002\n",
      "\n",
      " [INFO] Round 279\n",
      "Val loss: 1.0578031539916992, Val accuracy: 0.7067705988883972\n",
      "\n",
      " [INFO] Round 280\n",
      "Val loss: 1.0570387840270996, Val accuracy: 0.7075191736221313\n",
      "\n",
      " [INFO] Round 281\n",
      "Val loss: 1.0561381578445435, Val accuracy: 0.7088097929954529\n",
      "\n",
      " [INFO] Round 282\n",
      "Val loss: 1.059593677520752, Val accuracy: 0.7078547477722168\n",
      "\n",
      " [INFO] Round 283\n",
      "Val loss: 1.0535638332366943, Val accuracy: 0.7092227935791016\n",
      "\n",
      " [INFO] Round 284\n",
      "Val loss: 1.057210922241211, Val accuracy: 0.7078031301498413\n",
      "\n",
      " [INFO] Round 285\n",
      "Val loss: 1.0670311450958252, Val accuracy: 0.7043700218200684\n",
      "\n",
      " [INFO] Round 286\n",
      "Val loss: 1.0678170919418335, Val accuracy: 0.7037505507469177\n",
      "\n",
      " [INFO] Round 287\n",
      "Val loss: 1.0558794736862183, Val accuracy: 0.7067705988883972\n",
      "\n",
      " [INFO] Round 288\n",
      "Val loss: 1.05301034450531, Val accuracy: 0.7095325589179993\n",
      "\n",
      " [INFO] Round 289\n",
      "Val loss: 1.0472148656845093, Val accuracy: 0.711391031742096\n",
      "\n",
      " [INFO] Round 290\n",
      "Val loss: 1.0526982545852661, Val accuracy: 0.7103843688964844\n",
      "\n",
      " [INFO] Round 291\n",
      "Val loss: 1.0447003841400146, Val accuracy: 0.7125009894371033\n",
      "\n",
      " [INFO] Round 292\n",
      "Val loss: 1.0486440658569336, Val accuracy: 0.7099713683128357\n",
      "\n",
      " [INFO] Round 293\n",
      "Val loss: 1.044113039970398, Val accuracy: 0.7114942669868469\n",
      "\n",
      " [INFO] Round 294\n",
      "Val loss: 1.0469971895217896, Val accuracy: 0.711003839969635\n",
      "\n",
      " [INFO] Round 295\n",
      "Val loss: 1.0465149879455566, Val accuracy: 0.7100487947463989\n",
      "\n",
      " [INFO] Round 296\n",
      "Val loss: 1.0477378368377686, Val accuracy: 0.7094550728797913\n",
      "\n",
      " [INFO] Round 297\n",
      "Val loss: 1.0415986776351929, Val accuracy: 0.7117524147033691\n",
      "\n",
      " [INFO] Round 298\n",
      "Val loss: 1.0373917818069458, Val accuracy: 0.7121137976646423\n",
      "\n",
      " [INFO] Round 299\n",
      "Val loss: 1.0508008003234863, Val accuracy: 0.7086549401283264\n",
      "\n",
      " [INFO] Round 300\n",
      "Val loss: 1.043979525566101, Val accuracy: 0.7105392217636108\n",
      "\n",
      " [INFO] Round 301\n",
      "Val loss: 1.0402783155441284, Val accuracy: 0.7122686505317688\n",
      "\n",
      " [INFO] Round 302\n",
      "Val loss: 1.0356123447418213, Val accuracy: 0.7131720781326294\n",
      "\n",
      " [INFO] Round 303\n",
      "Val loss: 1.0345581769943237, Val accuracy: 0.7128106951713562\n",
      "\n",
      " [INFO] Round 304\n",
      "Val loss: 1.0399199724197388, Val accuracy: 0.7102552652359009\n",
      "\n",
      " [INFO] Round 305\n",
      "Val loss: 1.0372509956359863, Val accuracy: 0.7134560346603394\n",
      "\n",
      " [INFO] Round 306\n",
      "Val loss: 1.0360616445541382, Val accuracy: 0.712630033493042\n",
      "\n",
      " [INFO] Round 307\n",
      "Val loss: 1.0320806503295898, Val accuracy: 0.7143594622612\n",
      "\n",
      " [INFO] Round 308\n",
      "Val loss: 1.0331288576126099, Val accuracy: 0.7140496969223022\n",
      "\n",
      " [INFO] Round 309\n",
      "Val loss: 1.0336863994598389, Val accuracy: 0.7136108875274658\n",
      "\n",
      " [INFO] Round 310\n",
      "Val loss: 1.0383973121643066, Val accuracy: 0.7149015069007874\n",
      "\n",
      " [INFO] Round 311\n",
      "Val loss: 1.0446441173553467, Val accuracy: 0.714152991771698\n",
      "\n",
      " [INFO] Round 312\n",
      "Val loss: 1.0357385873794556, Val accuracy: 0.7139206528663635\n",
      "\n",
      " [INFO] Round 313\n",
      "Val loss: 1.0306665897369385, Val accuracy: 0.7147982716560364\n",
      "\n",
      " [INFO] Round 314\n",
      "Val loss: 1.0373903512954712, Val accuracy: 0.7132236957550049\n",
      "\n",
      " [INFO] Round 315\n",
      "Val loss: 1.0987201929092407, Val accuracy: 0.703827977180481\n",
      "\n",
      " [INFO] Round 316\n",
      "Val loss: 1.0277926921844482, Val accuracy: 0.71407550573349\n",
      "\n",
      " [INFO] Round 317\n",
      "Val loss: 1.0264766216278076, Val accuracy: 0.7142304182052612\n",
      "\n",
      " [INFO] Round 318\n",
      "Val loss: 1.0246312618255615, Val accuracy: 0.7137141823768616\n",
      "\n",
      " [INFO] Round 319\n",
      "Val loss: 1.0250226259231567, Val accuracy: 0.715314507484436\n",
      "\n",
      " [INFO] Round 320\n",
      "Val loss: 1.0241776704788208, Val accuracy: 0.7164502739906311\n",
      "\n",
      " [INFO] Round 321\n",
      "Val loss: 1.0238261222839355, Val accuracy: 0.7167858481407166\n",
      "\n",
      " [INFO] Round 322\n",
      "Val loss: 1.0193665027618408, Val accuracy: 0.718773365020752\n",
      "\n",
      " [INFO] Round 323\n",
      "Val loss: 1.030797004699707, Val accuracy: 0.713972270488739\n",
      "\n",
      " [INFO] Round 324\n",
      "Val loss: 1.0152087211608887, Val accuracy: 0.7183345556259155\n",
      "\n",
      " [INFO] Round 325\n",
      "Val loss: 1.0185041427612305, Val accuracy: 0.717327892780304\n",
      "\n",
      " [INFO] Round 326\n",
      "Val loss: 1.0190528631210327, Val accuracy: 0.7165793180465698\n",
      "\n",
      " [INFO] Round 327\n",
      "Val loss: 1.026355266571045, Val accuracy: 0.7140238881111145\n",
      "\n",
      " [INFO] Round 328\n",
      "Val loss: 1.0128216743469238, Val accuracy: 0.7183345556259155\n",
      "\n",
      " [INFO] Round 329\n",
      "Val loss: 1.0111631155014038, Val accuracy: 0.7187475562095642\n",
      "\n",
      " [INFO] Round 330\n",
      "Val loss: 1.0066001415252686, Val accuracy: 0.7212255597114563\n",
      "\n",
      " [INFO] Round 331\n",
      "Val loss: 1.0125657320022583, Val accuracy: 0.7176376581192017\n",
      "\n",
      " [INFO] Round 332\n",
      "Val loss: 1.0128525495529175, Val accuracy: 0.7175860404968262\n",
      "\n",
      " [INFO] Round 333\n",
      "Val loss: 1.013081431388855, Val accuracy: 0.71828293800354\n",
      "\n",
      " [INFO] Round 334\n",
      "Val loss: 1.0777231454849243, Val accuracy: 0.7001625895500183\n",
      "\n",
      " [INFO] Round 335\n",
      "Val loss: 1.021683931350708, Val accuracy: 0.7158566117286682\n",
      "\n",
      " [INFO] Round 336\n",
      "Val loss: 1.0322532653808594, Val accuracy: 0.7138432264328003\n",
      "\n",
      " [INFO] Round 337\n",
      "Val loss: 1.0128123760223389, Val accuracy: 0.7201414704322815\n",
      "\n",
      " [INFO] Round 338\n",
      "Val loss: 1.009577751159668, Val accuracy: 0.7211739420890808\n",
      "\n",
      " [INFO] Round 339\n",
      "Val loss: 1.0099663734436035, Val accuracy: 0.721535325050354\n",
      "\n",
      " [INFO] Round 340\n",
      "Val loss: 1.0094783306121826, Val accuracy: 0.7217159867286682\n",
      "\n",
      " [INFO] Round 341\n",
      "Val loss: 1.0325968265533447, Val accuracy: 0.7142304182052612\n",
      "\n",
      " [INFO] Round 342\n",
      "Val loss: 1.0082780122756958, Val accuracy: 0.7226194739341736\n",
      "\n",
      " [INFO] Round 343\n",
      "Val loss: 1.0104591846466064, Val accuracy: 0.7203479409217834\n",
      "\n",
      " [INFO] Round 344\n",
      "Val loss: 1.012966275215149, Val accuracy: 0.7191089391708374\n",
      "\n",
      " [INFO] Round 345\n",
      "Val loss: 1.0066850185394287, Val accuracy: 0.7194961309432983\n",
      "\n",
      " [INFO] Round 346\n",
      "Val loss: 1.0152761936187744, Val accuracy: 0.7183862328529358\n",
      "\n",
      " [INFO] Round 347\n",
      "Val loss: 1.003745436668396, Val accuracy: 0.7205802798271179\n",
      "\n",
      " [INFO] Round 348\n",
      "Val loss: 1.01850163936615, Val accuracy: 0.7155210375785828\n",
      "\n",
      " [INFO] Round 349\n",
      "Val loss: 0.997624933719635, Val accuracy: 0.7219741344451904\n",
      "\n",
      " [INFO] Round 350\n",
      "Val loss: 1.0159064531326294, Val accuracy: 0.7171472311019897\n",
      "\n",
      " [INFO] Round 351\n",
      "Val loss: 1.0022886991500854, Val accuracy: 0.7205544710159302\n",
      "\n",
      " [INFO] Round 352\n",
      "Val loss: 1.0020112991333008, Val accuracy: 0.7205802798271179\n",
      "\n",
      " [INFO] Round 353\n",
      "Val loss: 0.9975494146347046, Val accuracy: 0.7219225168228149\n",
      "\n",
      " [INFO] Round 354\n",
      "Val loss: 1.0014746189117432, Val accuracy: 0.721741795539856\n",
      "\n",
      " [INFO] Round 355\n",
      "Val loss: 0.9995738863945007, Val accuracy: 0.7222064733505249\n",
      "\n",
      " [INFO] Round 356\n",
      "Val loss: 0.9954568147659302, Val accuracy: 0.7239100933074951\n",
      "\n",
      " [INFO] Round 357\n",
      "Val loss: 0.9975778460502625, Val accuracy: 0.7237809896469116\n",
      "\n",
      " [INFO] Round 358\n",
      "Val loss: 0.99457848072052, Val accuracy: 0.7235229015350342\n",
      "\n",
      " [INFO] Round 359\n",
      "Val loss: 0.9976209998130798, Val accuracy: 0.7237809896469116\n",
      "\n",
      " [INFO] Round 360\n",
      "Val loss: 0.9992902874946594, Val accuracy: 0.7236519455909729\n",
      "\n",
      " [INFO] Round 361\n",
      "Val loss: 1.0044053792953491, Val accuracy: 0.7208125591278076\n",
      "\n",
      " [INFO] Round 362\n",
      "Val loss: 1.0034915208816528, Val accuracy: 0.7204511761665344\n",
      "\n",
      " [INFO] Round 363\n",
      "Val loss: 0.9995641112327576, Val accuracy: 0.7224387526512146\n",
      "\n",
      " [INFO] Round 364\n",
      "Val loss: 1.0146369934082031, Val accuracy: 0.7220773696899414\n",
      "\n",
      " [INFO] Round 365\n",
      "Val loss: 0.9905961751937866, Val accuracy: 0.7250458002090454\n",
      "\n",
      " [INFO] Round 366\n",
      "Val loss: 0.9997355341911316, Val accuracy: 0.7213029861450195\n",
      "\n",
      " [INFO] Round 367\n",
      "Val loss: 0.9880076050758362, Val accuracy: 0.7253297567367554\n",
      "\n",
      " [INFO] Round 368\n",
      "Val loss: 1.0192737579345703, Val accuracy: 0.7162696123123169\n",
      "\n",
      " [INFO] Round 369\n",
      "Val loss: 1.03986394405365, Val accuracy: 0.7078031301498413\n",
      "\n",
      " [INFO] Round 370\n",
      "Val loss: 0.9838100671768188, Val accuracy: 0.722980797290802\n",
      "\n",
      " [INFO] Round 371\n",
      "Val loss: 0.9834056496620178, Val accuracy: 0.7239617109298706\n",
      "\n",
      " [INFO] Round 372\n",
      "Val loss: 1.0195777416229248, Val accuracy: 0.7157533168792725\n",
      "\n",
      " [INFO] Round 373\n",
      "Val loss: 0.9836944937705994, Val accuracy: 0.7229033708572388\n",
      "\n",
      " [INFO] Round 374\n",
      "Val loss: 0.9975218772888184, Val accuracy: 0.722980797290802\n",
      "\n",
      " [INFO] Round 375\n",
      "Val loss: 0.9931063652038574, Val accuracy: 0.723264753818512\n",
      "\n",
      " [INFO] Round 376\n",
      "Val loss: 0.9880115389823914, Val accuracy: 0.7243489027023315\n",
      "\n",
      " [INFO] Round 377\n",
      "Val loss: 0.9863518476486206, Val accuracy: 0.7263622283935547\n",
      "\n",
      " [INFO] Round 378\n",
      "Val loss: 0.982153594493866, Val accuracy: 0.7268527150154114\n",
      "\n",
      " [INFO] Round 379\n",
      "Val loss: 0.9938804507255554, Val accuracy: 0.7232905626296997\n",
      "\n",
      " [INFO] Round 380\n",
      "Val loss: 0.9856728911399841, Val accuracy: 0.7248393297195435\n",
      "\n",
      " [INFO] Round 381\n",
      "Val loss: 0.9892643690109253, Val accuracy: 0.724890947341919\n",
      "\n",
      " [INFO] Round 382\n",
      "Val loss: 0.9784680008888245, Val accuracy: 0.7299501895904541\n",
      "\n",
      " [INFO] Round 383\n",
      "Val loss: 0.9839980006217957, Val accuracy: 0.7257169485092163\n",
      "\n",
      " [INFO] Round 384\n",
      "Val loss: 0.9796554446220398, Val accuracy: 0.7277045249938965\n",
      "\n",
      " [INFO] Round 385\n",
      "Val loss: 0.995168924331665, Val accuracy: 0.7226969003677368\n",
      "\n",
      " [INFO] Round 386\n",
      "Val loss: 0.9850240349769592, Val accuracy: 0.7252781391143799\n",
      "\n",
      " [INFO] Round 387\n",
      "Val loss: 0.9745433926582336, Val accuracy: 0.7285821437835693\n",
      "\n",
      " [INFO] Round 388\n",
      "Val loss: 0.9755326509475708, Val accuracy: 0.7287886142730713\n",
      "\n",
      " [INFO] Round 389\n",
      "Val loss: 0.9771398305892944, Val accuracy: 0.7276528477668762\n",
      "\n",
      " [INFO] Round 390\n",
      "Val loss: 0.9899311065673828, Val accuracy: 0.7246586084365845\n",
      "\n",
      " [INFO] Round 391\n",
      "Val loss: 0.9730868935585022, Val accuracy: 0.7302341461181641\n",
      "\n",
      " [INFO] Round 392\n",
      "Val loss: 0.9758713841438293, Val accuracy: 0.7294855713844299\n",
      "\n",
      " [INFO] Round 393\n",
      "Val loss: 0.9718784689903259, Val accuracy: 0.7294855713844299\n",
      "\n",
      " [INFO] Round 394\n",
      "Val loss: 0.9913424253463745, Val accuracy: 0.7230066061019897\n",
      "\n",
      " [INFO] Round 395\n",
      "Val loss: 0.9755293726921082, Val accuracy: 0.7266461849212646\n",
      "\n",
      " [INFO] Round 396\n",
      "Val loss: 0.9668875932693481, Val accuracy: 0.730156660079956\n",
      "\n",
      " [INFO] Round 397\n",
      "Val loss: 0.970348060131073, Val accuracy: 0.7294855713844299\n",
      "\n",
      " [INFO] Round 398\n",
      "Val loss: 0.9814183712005615, Val accuracy: 0.7239617109298706\n",
      "\n",
      " [INFO] Round 399\n",
      "Val loss: 0.97714763879776, Val accuracy: 0.7280142307281494\n",
      "\n",
      " [INFO] Round 400\n",
      "Val loss: 0.9682116508483887, Val accuracy: 0.7286853790283203\n",
      "\n",
      " [INFO] Round 401\n",
      "Val loss: 0.9648218750953674, Val accuracy: 0.7298985719680786\n",
      "\n",
      " [INFO] Round 402\n",
      "Val loss: 0.9646342396736145, Val accuracy: 0.7314989566802979\n",
      "\n",
      " [INFO] Round 403\n",
      "Val loss: 0.9974077939987183, Val accuracy: 0.7249941825866699\n",
      "\n",
      " [INFO] Round 404\n",
      "Val loss: 0.9662727117538452, Val accuracy: 0.7308536171913147\n",
      "\n",
      " [INFO] Round 405\n",
      "Val loss: 0.9679844975471497, Val accuracy: 0.7296662330627441\n",
      "\n",
      " [INFO] Round 406\n",
      "Val loss: 0.9631062746047974, Val accuracy: 0.7319119572639465\n",
      "\n",
      " [INFO] Round 407\n",
      "Val loss: 0.9750509858131409, Val accuracy: 0.7265687584877014\n",
      "\n",
      " [INFO] Round 408\n",
      "Val loss: 0.9768955707550049, Val accuracy: 0.7255362272262573\n",
      "\n",
      " [INFO] Round 409\n",
      "Val loss: 0.9661996960639954, Val accuracy: 0.7300792336463928\n",
      "\n",
      " [INFO] Round 410\n",
      "Val loss: 0.9592829942703247, Val accuracy: 0.731963574886322\n",
      "\n",
      " [INFO] Round 411\n",
      "Val loss: 0.9592189788818359, Val accuracy: 0.7322990894317627\n",
      "\n",
      " [INFO] Round 412\n",
      "Val loss: 0.9588149189949036, Val accuracy: 0.7319893836975098\n",
      "\n",
      " [INFO] Round 413\n",
      "Val loss: 0.9572401642799377, Val accuracy: 0.7325830459594727\n",
      "\n",
      " [INFO] Round 414\n",
      "Val loss: 0.957958459854126, Val accuracy: 0.7327637672424316\n",
      "\n",
      " [INFO] Round 415\n",
      "Val loss: 0.9648280143737793, Val accuracy: 0.7321958541870117\n",
      "\n",
      " [INFO] Round 416\n",
      "Val loss: 0.9580450057983398, Val accuracy: 0.7337704300880432\n",
      "\n",
      " [INFO] Round 417\n",
      "Val loss: 0.9609300494194031, Val accuracy: 0.7316796183586121\n",
      "\n",
      " [INFO] Round 418\n",
      "Val loss: 0.9590981602668762, Val accuracy: 0.7316538095474243\n",
      "\n",
      " [INFO] Round 419\n",
      "Val loss: 0.9575141072273254, Val accuracy: 0.7326862812042236\n",
      "\n",
      " [INFO] Round 420\n",
      "Val loss: 0.9578348398208618, Val accuracy: 0.7317312359809875\n",
      "\n",
      " [INFO] Round 421\n",
      "Val loss: 0.955303430557251, Val accuracy: 0.7332283854484558\n",
      "\n",
      " [INFO] Round 422\n",
      "Val loss: 0.9860441088676453, Val accuracy: 0.7261815667152405\n",
      "\n",
      " [INFO] Round 423\n",
      "Val loss: 0.9558846950531006, Val accuracy: 0.7339252829551697\n",
      "\n",
      " [INFO] Round 424\n",
      "Val loss: 0.9682782888412476, Val accuracy: 0.7311633825302124\n",
      "\n",
      " [INFO] Round 425\n",
      "Val loss: 1.0547977685928345, Val accuracy: 0.7083451747894287\n",
      "\n",
      " [INFO] Round 426\n",
      "Val loss: 0.96707683801651, Val accuracy: 0.7294081449508667\n",
      "\n",
      " [INFO] Round 427\n",
      "Val loss: 0.9601995348930359, Val accuracy: 0.7316280007362366\n",
      "\n",
      " [INFO] Round 428\n",
      "Val loss: 0.9614134430885315, Val accuracy: 0.7309826612472534\n",
      "\n",
      " [INFO] Round 429\n",
      "Val loss: 0.9586501717567444, Val accuracy: 0.7318602800369263\n",
      "\n",
      " [INFO] Round 430\n",
      "Val loss: 0.9620873928070068, Val accuracy: 0.7306471467018127\n",
      "\n",
      " [INFO] Round 431\n",
      "Val loss: 0.9532949924468994, Val accuracy: 0.7344415187835693\n",
      "\n",
      " [INFO] Round 432\n",
      "Val loss: 0.9533961415290833, Val accuracy: 0.7325830459594727\n",
      "\n",
      " [INFO] Round 433\n",
      "Val loss: 0.9543692469596863, Val accuracy: 0.7312666177749634\n",
      "\n",
      " [INFO] Round 434\n",
      "Val loss: 0.9576411843299866, Val accuracy: 0.7314989566802979\n",
      "\n",
      " [INFO] Round 435\n",
      "Val loss: 0.9488108158111572, Val accuracy: 0.7329960465431213\n",
      "\n",
      " [INFO] Round 436\n",
      "Val loss: 0.9499645829200745, Val accuracy: 0.7340801954269409\n",
      "\n",
      " [INFO] Round 437\n",
      "Val loss: 0.9451648592948914, Val accuracy: 0.7346480488777161\n",
      "\n",
      " [INFO] Round 438\n",
      "Val loss: 0.9782133102416992, Val accuracy: 0.7285305261611938\n",
      "\n",
      " [INFO] Round 439\n",
      "Val loss: 0.9448457360267639, Val accuracy: 0.7357580065727234\n",
      "\n",
      " [INFO] Round 440\n",
      "Val loss: 0.9539017081260681, Val accuracy: 0.7313440442085266\n",
      "\n",
      " [INFO] Round 441\n",
      "Val loss: 0.9670087695121765, Val accuracy: 0.7304922342300415\n",
      "\n",
      " [INFO] Round 442\n",
      "Val loss: 0.947407066822052, Val accuracy: 0.7344931960105896\n",
      "\n",
      " [INFO] Round 443\n",
      "Val loss: 0.9568880796432495, Val accuracy: 0.7340285778045654\n",
      "\n",
      " [INFO] Round 444\n",
      "Val loss: 0.9411036372184753, Val accuracy: 0.7361710071563721\n",
      "\n",
      " [INFO] Round 445\n",
      "Val loss: 0.9575353860855103, Val accuracy: 0.7304148077964783\n",
      "\n",
      " [INFO] Round 446\n",
      "Val loss: 0.9478095769882202, Val accuracy: 0.7324281930923462\n",
      "\n",
      " [INFO] Round 447\n",
      "Val loss: 0.9434037804603577, Val accuracy: 0.7355256676673889\n",
      "\n",
      " [INFO] Round 448\n",
      "Val loss: 0.9351757764816284, Val accuracy: 0.736661434173584\n",
      "\n",
      " [INFO] Round 449\n",
      "Val loss: 0.9429295063018799, Val accuracy: 0.7360935211181641\n",
      "\n",
      " [INFO] Round 450\n",
      "Val loss: 0.9402854442596436, Val accuracy: 0.7366872429847717\n",
      "\n",
      " [INFO] Round 451\n",
      "Val loss: 0.9441790580749512, Val accuracy: 0.7371518611907959\n",
      "\n",
      " [INFO] Round 452\n",
      "Val loss: 0.9433729648590088, Val accuracy: 0.7340285778045654\n",
      "\n",
      " [INFO] Round 453\n",
      "Val loss: 0.9319233298301697, Val accuracy: 0.7371776700019836\n",
      "\n",
      " [INFO] Round 454\n",
      "Val loss: 0.9496303200721741, Val accuracy: 0.735319197177887\n",
      "\n",
      " [INFO] Round 455\n",
      "Val loss: 0.9377503395080566, Val accuracy: 0.7364032864570618\n",
      "\n",
      " [INFO] Round 456\n",
      "Val loss: 0.9456886053085327, Val accuracy: 0.7344931960105896\n",
      "\n",
      " [INFO] Round 457\n",
      "Val loss: 0.931563138961792, Val accuracy: 0.7381327152252197\n",
      "\n",
      " [INFO] Round 458\n",
      "Val loss: 0.934283435344696, Val accuracy: 0.7374874353408813\n",
      "\n",
      " [INFO] Round 459\n",
      "Val loss: 0.9412358403205872, Val accuracy: 0.7351642847061157\n",
      "\n",
      " [INFO] Round 460\n",
      "Val loss: 0.9270251989364624, Val accuracy: 0.7380294799804688\n",
      "\n",
      " [INFO] Round 461\n",
      "Val loss: 0.9343610405921936, Val accuracy: 0.7361451983451843\n",
      "\n",
      " [INFO] Round 462\n",
      "Val loss: 0.9430109858512878, Val accuracy: 0.735138475894928\n",
      "\n",
      " [INFO] Round 463\n",
      "Val loss: 0.9284385442733765, Val accuracy: 0.7385457158088684\n",
      "\n",
      " [INFO] Round 464\n",
      "Val loss: 0.9283682703971863, Val accuracy: 0.7403525710105896\n",
      "\n",
      " [INFO] Round 465\n",
      "Val loss: 0.9316845536231995, Val accuracy: 0.7391394376754761\n",
      "\n",
      " [INFO] Round 466\n",
      "Val loss: 0.932980477809906, Val accuracy: 0.7378746271133423\n",
      "\n",
      " [INFO] Round 467\n",
      "Val loss: 0.9276348352432251, Val accuracy: 0.739733099937439\n",
      "\n",
      " [INFO] Round 468\n",
      "Val loss: 0.9876027703285217, Val accuracy: 0.7269559502601624\n",
      "\n",
      " [INFO] Round 469\n",
      "Val loss: 0.9331102967262268, Val accuracy: 0.7398363351821899\n",
      "\n",
      " [INFO] Round 470\n",
      "Val loss: 0.9364154934883118, Val accuracy: 0.737719714641571\n",
      "\n",
      " [INFO] Round 471\n",
      "Val loss: 1.0039831399917603, Val accuracy: 0.7214578986167908\n",
      "\n",
      " [INFO] Round 472\n",
      "Val loss: 0.9363791942596436, Val accuracy: 0.73790043592453\n",
      "\n",
      " [INFO] Round 473\n",
      "Val loss: 0.9406073689460754, Val accuracy: 0.7365065217018127\n",
      "\n",
      " [INFO] Round 474\n",
      "Val loss: 0.9335695505142212, Val accuracy: 0.7385973334312439\n",
      "\n",
      " [INFO] Round 475\n",
      "Val loss: 0.9296255707740784, Val accuracy: 0.7398621439933777\n",
      "\n",
      " [INFO] Round 476\n",
      "Val loss: 0.929153323173523, Val accuracy: 0.7394233345985413\n",
      "\n",
      " [INFO] Round 477\n",
      "Val loss: 0.9338851571083069, Val accuracy: 0.7386747598648071\n",
      "\n",
      " [INFO] Round 478\n",
      "Val loss: 0.9335545301437378, Val accuracy: 0.7392942905426025\n",
      "\n",
      " [INFO] Round 479\n",
      "Val loss: 0.9291298389434814, Val accuracy: 0.7399654388427734\n",
      "\n",
      " [INFO] Round 480\n",
      "Val loss: 0.9279900193214417, Val accuracy: 0.7391652464866638\n",
      "\n",
      " [INFO] Round 481\n",
      "Val loss: 0.9237028360366821, Val accuracy: 0.7407139539718628\n",
      "\n",
      " [INFO] Round 482\n",
      "Val loss: 0.9281885623931885, Val accuracy: 0.7389070987701416\n",
      "\n",
      " [INFO] Round 483\n",
      "Val loss: 0.9245496392250061, Val accuracy: 0.7405332922935486\n",
      "\n",
      " [INFO] Round 484\n",
      "Val loss: 0.9284413456916809, Val accuracy: 0.7378746271133423\n",
      "\n",
      " [INFO] Round 485\n",
      "Val loss: 0.9232451915740967, Val accuracy: 0.7396814823150635\n",
      "\n",
      " [INFO] Round 486\n",
      "Val loss: 0.930458664894104, Val accuracy: 0.7388812899589539\n",
      "\n",
      " [INFO] Round 487\n",
      "Val loss: 0.9261064529418945, Val accuracy: 0.7406623363494873\n",
      "\n",
      " [INFO] Round 488\n",
      "Val loss: 0.9184292554855347, Val accuracy: 0.7418497204780579\n",
      "\n",
      " [INFO] Round 489\n",
      "Val loss: 0.9216790795326233, Val accuracy: 0.7411527633666992\n",
      "\n",
      " [INFO] Round 490\n",
      "Val loss: 0.9225744605064392, Val accuracy: 0.7403525710105896\n",
      "\n",
      " [INFO] Round 491\n",
      "Val loss: 0.9245597720146179, Val accuracy: 0.7408946752548218\n",
      "\n",
      " [INFO] Round 492\n",
      "Val loss: 0.9236645102500916, Val accuracy: 0.7403009533882141\n",
      "\n",
      " [INFO] Round 493\n",
      "Val loss: 0.9241346120834351, Val accuracy: 0.7403009533882141\n",
      "\n",
      " [INFO] Round 494\n",
      "Val loss: 0.9210219979286194, Val accuracy: 0.741075336933136\n",
      "\n",
      " [INFO] Round 495\n",
      "Val loss: 0.9320776462554932, Val accuracy: 0.7369453310966492\n",
      "\n",
      " [INFO] Round 496\n",
      "Val loss: 0.9225130677223206, Val accuracy: 0.7394233345985413\n",
      "\n",
      " [INFO] Round 497\n",
      "Val loss: 0.9194942116737366, Val accuracy: 0.7409204840660095\n",
      "\n",
      " [INFO] Round 498\n",
      "Val loss: 0.921747624874115, Val accuracy: 0.7411011457443237\n",
      "\n",
      " [INFO] Round 499\n",
      "Val loss: 0.9209800362586975, Val accuracy: 0.7416432499885559\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = num_clients\n",
    "list_val_acc = []\n",
    "list_val_loss = []\n",
    "\n",
    "\n",
    "for idx_round in range(NUM_ROUNDS):\n",
    "    print(\"\\n [INFO] Round {}\".format(idx_round))\n",
    "\n",
    "    if (idx_round > MAX_PRUNED_ROUND) and (IS_STILL_PRUNE == True):\n",
    "        IS_STILL_PRUNE = False\n",
    "        print(f\"===== [INFO] Stop prune here! =====\")\n",
    "        print(f\"Final params: {global_model.count_params()}\")\n",
    "\n",
    "    if (0 < idx_round) and (IS_STILL_PRUNE == True):\n",
    "        global_model = prune_model(global_model, optimizer=OPTIMIZER, loss_func=LOSS, metrics=METRICS, std_threshold=STD_THRESHOLD_PRUNE)\n",
    "\n",
    "        client_model = keras.models.clone_model(global_model)    \n",
    "        client_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "\n",
    "    # Select random subset of clients\n",
    "    selected_percent_client = np.random.uniform(0.0, MAX_SELECTED_PERCENT_CLIENT)\n",
    "    num_selected_clients = max(int(NUM_CLIENTS * selected_percent_client), 1)\n",
    "    selected_clients_data = random.sample(list_clients_data, num_selected_clients)\n",
    "\n",
    "    # Loop through selected client\n",
    "    list_client_model_weight = []\n",
    "    list_client_scales = []\n",
    "    for selectd_client_data in selected_clients_data:      \n",
    "        client_model.set_weights(global_model.get_weights())  # Clone global model\n",
    "\n",
    "        list_X = selectd_client_data['list_X']\n",
    "        list_y = selectd_client_data['list_y']\n",
    "        client_model.fit(list_X, list_y, epochs=LOCAL_EPOCHS, batch_size=LOCAL_BATCH_SIZE, verbose=0, validation_split=0.05)\n",
    "\n",
    "        list_client_model_weight.append(client_model.get_weights())    # store local weight for update global model later.\n",
    "        list_client_scales.append(len(list_X))\n",
    "    \n",
    "    # Calculate scale of each client\n",
    "    list_client_scales = np.array(list_client_scales)\n",
    "    list_client_scales = list_client_scales / list_client_scales.sum()\n",
    "\n",
    "    # Update the global model weights\n",
    "    avg_weights = FedAvg(global_model, list_client_model_weight, list_client_scales)\n",
    "    global_model.set_weights(avg_weights)\n",
    "\n",
    "    # Evaluate model on validation data\n",
    "    val_loss, val_acc = global_model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')\n",
    "    list_val_acc.append(val_acc)\n",
    "    list_val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X test: (38742, 28, 28, 1)\n",
      "Shape of y test: (38742, 62)\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.array([resize(image, (IMAGE_DIMENSION, IMAGE_DIMENSION)) for image in X_test])\n",
    "print(f\"Shape of X test: {X_test.shape}\")\n",
    "print(f\"Shape of y test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9207684397697449, Val accuracy: 0.7361519932746887\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on testing data\n",
    "val_loss, val_acc = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afte pruning:\n",
      "Number of params: 21902\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prunable_conv_0 (Conv2D)    (None, 24, 24, 24)        624       \n",
      "                                                                 \n",
      " activation_74 (Activation)  (None, 24, 24, 24)        0         \n",
      "                                                                 \n",
      " max_pooling2d_74 (MaxPooli  (None, 12, 12, 24)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " prunable_conv_1 (Conv2D)    (None, 8, 8, 32)          19232     \n",
      "                                                                 \n",
      " activation_75 (Activation)  (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d_75 (MaxPooli  (None, 4, 4, 32)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_3  (None, 32)                0         \n",
      " 7 (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " classifier (Dense)          (None, 62)                2046      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21902 (85.55 KB)\n",
      "Trainable params: 21902 (85.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Afte pruning:\")\n",
    "print(f\"Number of params: {global_model.count_params()}\")\n",
    "global_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
